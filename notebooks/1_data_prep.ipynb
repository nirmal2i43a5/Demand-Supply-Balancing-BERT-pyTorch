{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets accelerate evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19292f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from evaluate import load as load_metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5b23f",
   "metadata": {},
   "source": [
    "### Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d5ec466",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"  \n",
    "MODEL_NAME = \"bert-base-cased\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae6d24",
   "metadata": {},
   "source": [
    "### Load label list & mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cda661ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_list: List[str] = [l.strip() for l in open(os.path.join(DATA_DIR, \"labels.txt\"), encoding=\"utf-8\")]\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c17e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O', 1: 'B-CHEMICAL', 2: 'I-CHEMICAL', 3: 'B-DISEASE', 4: 'I-DISEASE'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ef921fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B-CHEMICAL': 1, 'I-CHEMICAL': 2, 'B-DISEASE': 3, 'I-DISEASE': 4}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "370a4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_conll(path: str):\n",
    "    \n",
    "    sentences, tags = [], []\n",
    "    cur_tokens, cur_tags = [], []\n",
    "    \n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if cur_tokens:\n",
    "                    sentences.append(cur_tokens); tags.append(cur_tags)\n",
    "                    cur_tokens, cur_tags = [], []\n",
    "                continue\n",
    "            \n",
    "            # each line: \"<token> <tag>\"\n",
    "            parts = line.split()\n",
    "            token = parts[0]\n",
    "            tag = parts[-1]\n",
    "            cur_tokens.append(token)\n",
    "            cur_tags.append(tag)\n",
    "            \n",
    "    if cur_tokens:\n",
    "        sentences.append(cur_tokens); tags.append(cur_tags)\n",
    "        \n",
    "    return sentences, tags\n",
    "\n",
    "\n",
    "train_sents, train_tags = read_conll(os.path.join(DATA_DIR, \"bc5cdr_train.conll\"))\n",
    "val_sents,   val_tags   = read_conll(os.path.join(DATA_DIR, \"bc5cdr_validation.conll\"))\n",
    "test_sents,  test_tags  = read_conll(os.path.join(DATA_DIR, \"bc5cdr_test.conll\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbc46467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in HF datasets\n",
    "def to_hf_dataset(sents, tags):\n",
    "    return Dataset.from_dict({\"tokens\": sents, \"ner_tags\": tags})\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": to_hf_dataset(train_sents, train_tags),\n",
    "    \"validation\": to_hf_dataset(val_sents, val_tags),\n",
    "    \"test\": to_hf_dataset(test_sents, test_tags),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b68bf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'B-DISEASE', 'O', 'O']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "ner_tags = data['train'][0]['ner_tags']\n",
    "print(ner_tags)\n",
    "print(len(ner_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2c6f5",
   "metadata": {},
   "source": [
    "### Model Building (Tokenizer and model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125446b",
   "metadata": {},
   "source": [
    "#### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e7f3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fdc29c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6d44d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Antihypertensive', 'drugs', 'and', 'depression:', 'a', 'reappraisal.']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "tokens = data['train'][0]['tokens']#original tokens \n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52df1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data['train'][0]['tokens']\n",
    "inputs = tokenizer(inputs, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "976a73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8329, 7889, 17786, 5026, 2109, 5557, 1105, 7560, 131, 170, 1231, 11478, 20488, 15630, 1233, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'Anti', '##hy', '##pert', '##ens', '##ive', 'drugs', 'and', 'depression', ':', 'a', 're', '##ap', '##pra', '##isa', '##l', '.', '[SEP]']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(inputs.tokens())\n",
    "print(len(inputs.tokens()) - 2 )#removing [CLS] and [SEP ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43580529",
   "metadata": {},
   "source": [
    "- Here , we can analyse that after tokenizer our single tokens are divided into multiple (like depletion is \n",
    "divided into de, ##ple, ##tion).\n",
    "- So, now we need to solve this misalignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd47be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 1, 2, 3, 3, 4, 5, 5, 5, 5, 5, 5, None]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccb9b7",
   "metadata": {},
   "source": [
    "- We can see words ids (like 3,3,3) i.e this means for the single token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9abff",
   "metadata": {},
   "source": [
    "#### Preprocessing for Misaligned Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec6cb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    \n",
    "    for word_id in word_ids:\n",
    "        \n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "            \n",
    "        elif word_id != current_word:\n",
    "            current_word = word_id\n",
    "            new_labels.append(label2id[labels[word_id]])\n",
    "            \n",
    "        else:\n",
    "            new_labels.append(-100)#-100 means Don’t compute loss for this position.\n",
    "            #i.e ignore_index=-100 in PyTorch’s CrossEntropyLoss.\n",
    "            \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8fdcdb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'B-DISEASE', 'O', 'O']\n",
      "[None, 0, 0, 0, 0, 0, 1, 2, 3, 3, 4, 5, 5, 5, 5, 5, 5, None]\n"
     ]
    }
   ],
   "source": [
    "labels = data['train'][0]['ner_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(word_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "150e212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, -100, -100, -100, -100, 0, 0, 3, -100, 0, 0, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "new_labels_ids = align_labels_with_tokens(labels,word_ids)\n",
    "print(new_labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef68da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(batch):\n",
    "    \n",
    "    # print(batch)\n",
    "    \n",
    "    tokenized_inputs = tokenizer(batch[\"tokens\"], is_split_into_words=True)\n",
    "    label_ids = []\n",
    "    \n",
    "    for i, labels in enumerate(batch[\"ner_tags\"]):\n",
    "        \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = align_labels_with_tokens(labels, word_ids)#new label ids\n",
    "        label_ids.append(aligned_labels)\n",
    "        \n",
    "    # print(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    # print(f'Tokenized input : {tokenized_inputs}')\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d2456fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6212850cce43afb1c4b9b90e1d49e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5641ce856ed48e28bb8dacb48568b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4657 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56642f296b524a87959d7146a9060718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9a352de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4648\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4657\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55fa25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"tokens\", \"ner_tags\",\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752aaa5",
   "metadata": {},
   "source": [
    "For classification we only need input_ids, attention_mask and labels.So, columns can be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf91e6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4648\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4657\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "040cd701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a71eb093124da997e215b1b8dab554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bb36b99c0943b2b28b269eb22fb840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4657 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c50b82cbf074928a585be3e086acc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0dffac43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2f4c287f5d422ba1c3a4df2faf277e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0431d1e5a6a3465aa472a5e8f0f19b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4657 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e2c3b7296947658a41f25dff7e7225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets.save_to_disk(\"../data/processed/bio_ner\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
