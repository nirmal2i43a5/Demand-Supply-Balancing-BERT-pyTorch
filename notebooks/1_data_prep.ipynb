{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210273f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets accelerate evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b071c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5b23f",
   "metadata": {},
   "source": [
    "### Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5ec466",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"  \n",
    "MODEL_NAME = \"bert-base-cased\"  \n",
    "OUTPUT_DIR = \"outputs/bert_ner\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54322b31",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42cf949",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed) \n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed) \n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef0685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "    \"train\": \"../data/train.conll\",\n",
    "    \"validation\": \"../data/validation.conll\",\n",
    "    \"test\": \"../data/test.conll\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae6d24",
   "metadata": {},
   "source": [
    "### Load label list & mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda661ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_list: List[str] = [l.strip() for l in open(os.path.join(DATA_DIR, \"labels.txt\"), encoding=\"utf-8\")]\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c17e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-FACTORY_LOCATION',\n",
       " 2: 'I-FACTORY_LOCATION',\n",
       " 3: 'B-MACHINE_TYPE',\n",
       " 4: 'I-MACHINE_TYPE',\n",
       " 5: 'B-DOWNTIME_CAUSE',\n",
       " 6: 'I-DOWNTIME_CAUSE',\n",
       " 7: 'B-PRODUCTION_VOLUME',\n",
       " 8: 'I-PRODUCTION_VOLUME',\n",
       " 9: 'B-WORKFORCE_AVAILABILITY',\n",
       " 10: 'I-WORKFORCE_AVAILABILITY'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef921fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-FACTORY_LOCATION': 1,\n",
       " 'I-FACTORY_LOCATION': 2,\n",
       " 'B-MACHINE_TYPE': 3,\n",
       " 'I-MACHINE_TYPE': 4,\n",
       " 'B-DOWNTIME_CAUSE': 5,\n",
       " 'I-DOWNTIME_CAUSE': 6,\n",
       " 'B-PRODUCTION_VOLUME': 7,\n",
       " 'I-PRODUCTION_VOLUME': 8,\n",
       " 'B-WORKFORCE_AVAILABILITY': 9,\n",
       " 'I-WORKFORCE_AVAILABILITY': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "370a4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_conll(path: str):\n",
    "    sentences, tags = [], []\n",
    "    cur_tokens, cur_tags = [], []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if cur_tokens:\n",
    "                    sentences.append(cur_tokens); tags.append(cur_tags)\n",
    "                    cur_tokens, cur_tags = [], []\n",
    "                continue\n",
    "            \n",
    "            # each line: \"<token> <tag>\"\n",
    "            parts = line.split()\n",
    "            token = parts[0]\n",
    "            tag = parts[-1]\n",
    "            cur_tokens.append(token)\n",
    "            cur_tags.append(tag)\n",
    "    if cur_tokens:\n",
    "        sentences.append(cur_tokens); tags.append(cur_tags)\n",
    "    return sentences, tags\n",
    "\n",
    "train_sents, train_tags = read_conll(os.path.join(DATA_DIR, \"train.conll\"))\n",
    "val_sents,   val_tags   = read_conll(os.path.join(DATA_DIR, \"validation.conll\"))\n",
    "test_sents,  test_tags  = read_conll(os.path.join(DATA_DIR, \"test.conll\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbc46467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in HF datasets\n",
    "def to_hf_dataset(sents, tags):\n",
    "    return Dataset.from_dict({\"tokens\": sents, \"ner_tags\": tags})\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": to_hf_dataset(train_sents, train_tags),\n",
    "    \"validation\": to_hf_dataset(val_sents, val_tags),\n",
    "    \"test\": to_hf_dataset(test_sents, test_tags),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4916aff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-FACTORY_LOCATION', 'O', 'B-DOWNTIME_CAUSE', 'I-DOWNTIME_CAUSE', 'O', 'O', 'O', 'O', 'B-MACHINE_TYPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCTION_VOLUME', 'I-PRODUCTION_VOLUME', 'O', 'O', 'O', 'O']\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "ner_tags = data['train'][0]['ner_tags']\n",
    "print(ner_tags)\n",
    "print(len(ner_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2c6f5",
   "metadata": {},
   "source": [
    "### Model Building (Tokenizer and model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125446b",
   "metadata": {},
   "source": [
    "#### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e7f3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc29c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6d44d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Manitoba', 'experienced', 'reservoir', 'depletion', 'in', 'May', '2024', ';', 'intertie', 'capacity', 'tightened', 'and', 'authorities', 'recorded', 'a', '6.4%', 'decrease', 'relative', 'to', 'normal', 'levels.']\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "tokens = data['train'][0]['tokens']#original tokens \n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52df1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data['train'][0]['tokens']\n",
    "inputs = tokenizer(inputs, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "976a73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 10391, 4531, 10462, 1260, 7136, 2116, 1107, 1318, 17881, 1527, 132, 9455, 9570, 3211, 7974, 1105, 3912, 1802, 170, 127, 119, 125, 110, 9711, 5236, 1106, 2999, 3001, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'Manitoba', 'experienced', 'reservoir', 'de', '##ple', '##tion', 'in', 'May', '202', '##4', ';', 'inter', '##tie', 'capacity', 'tightened', 'and', 'authorities', 'recorded', 'a', '6', '.', '4', '%', 'decrease', 'relative', 'to', 'normal', 'levels', '.', '[SEP]']\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(inputs.tokens())\n",
    "print(len(inputs.tokens()) - 2 )#removing [CLS] and [SEP ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43580529",
   "metadata": {},
   "source": [
    "- Here , we can analyse that after tokenizer our single tokens are divided into multiple (like depletion is \n",
    "divided into de, ##ple, ##tion).\n",
    "- So, now we need to solve this misalignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd47be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 15, 15, 15, 15, 16, 17, 18, 19, 20, 20, None]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccb9b7",
   "metadata": {},
   "source": [
    "- We can see words ids (like 3,3,3) i.e this means for the single token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9abff",
   "metadata": {},
   "source": [
    "#### Preprocessing for Misaligned Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec6cb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    \n",
    "    for word_id in word_ids:\n",
    "        \n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "            \n",
    "        elif word_id != current_word:\n",
    "            current_word = word_id\n",
    "            new_labels.append(label2id[labels[word_id]])\n",
    "            \n",
    "        else:\n",
    "            new_labels.append(-100)#-100 means Don’t compute loss for this position.\n",
    "            #i.e ignore_index=-100 in PyTorch’s CrossEntropyLoss.\n",
    "            \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fdcdb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-FACTORY_LOCATION', 'O', 'B-DOWNTIME_CAUSE', 'I-DOWNTIME_CAUSE', 'O', 'O', 'O', 'O', 'B-MACHINE_TYPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCTION_VOLUME', 'I-PRODUCTION_VOLUME', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 15, 15, 15, 15, 16, 17, 18, 19, 20, 20, None]\n"
     ]
    }
   ],
   "source": [
    "labels = data['train'][0]['ner_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(word_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "150e212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 1, 0, 5, 6, -100, -100, 0, 0, 0, -100, 0, 3, -100, 0, 0, 0, 0, 0, 0, 7, -100, -100, -100, 8, 0, 0, 0, 0, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "new_labels_ids = align_labels_with_tokens(labels,word_ids)\n",
    "print(new_labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(batch):\n",
    "    \n",
    "    # print(batch)\n",
    "    \n",
    "    tokenized_inputs = tokenizer(batch[\"tokens\"], is_split_into_words=True)\n",
    "    label_ids = []\n",
    "    \n",
    "    for i, labels in enumerate(batch[\"ner_tags\"]):\n",
    "        \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = align_labels_with_tokens(labels, word_ids)#new label ids\n",
    "        label_ids.append(aligned_labels)\n",
    "        \n",
    "    # print(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    # print(f'Tokenized input : {tokenized_inputs}')\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d2456fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1601c5b0e344888695e1f8a0e360cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59b60ad2a65467ea576227d853cc141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e755b81040b84443886e3068fd29f404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9a352de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 110\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55fa25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"tokens\", \"ner_tags\",\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752aaa5",
   "metadata": {},
   "source": [
    "For classification we only need input_ids, attention_mask and labels.So, columns can be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf91e6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 110\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5b33174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0feb1d7a56864b93ac4ee65ee461deb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda63b6d57d748c788eeba9f5d944cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6c006601ee4b9fb31110800ea8bc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
