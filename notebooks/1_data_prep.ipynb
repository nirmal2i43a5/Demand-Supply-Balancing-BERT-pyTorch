{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210273f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets accelerate evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b071c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5b23f",
   "metadata": {},
   "source": [
    "### Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5ec466",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"  \n",
    "MODEL_NAME = \"bert-base-cased\"  \n",
    "OUTPUT_DIR = \"outputs/bert_ner\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54322b31",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42cf949",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed) \n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed) \n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef0685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "    \"train\": \"../data/train.conll\",\n",
    "    \"validation\": \"../data/validation.conll\",\n",
    "    \"test\": \"../data/test.conll\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae6d24",
   "metadata": {},
   "source": [
    "### Load label list & mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda661ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_list: List[str] = [l.strip() for l in open(os.path.join(DATA_DIR, \"labels.txt\"), encoding=\"utf-8\")]\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c17e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-FACTORY_LOCATION',\n",
       " 2: 'I-FACTORY_LOCATION',\n",
       " 3: 'B-MACHINE_TYPE',\n",
       " 4: 'I-MACHINE_TYPE',\n",
       " 5: 'B-DOWNTIME_CAUSE',\n",
       " 6: 'I-DOWNTIME_CAUSE',\n",
       " 7: 'B-PRODUCTION_VOLUME',\n",
       " 8: 'I-PRODUCTION_VOLUME',\n",
       " 9: 'B-WORKFORCE_AVAILABILITY',\n",
       " 10: 'I-WORKFORCE_AVAILABILITY'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef921fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-FACTORY_LOCATION': 1,\n",
       " 'I-FACTORY_LOCATION': 2,\n",
       " 'B-MACHINE_TYPE': 3,\n",
       " 'I-MACHINE_TYPE': 4,\n",
       " 'B-DOWNTIME_CAUSE': 5,\n",
       " 'I-DOWNTIME_CAUSE': 6,\n",
       " 'B-PRODUCTION_VOLUME': 7,\n",
       " 'I-PRODUCTION_VOLUME': 8,\n",
       " 'B-WORKFORCE_AVAILABILITY': 9,\n",
       " 'I-WORKFORCE_AVAILABILITY': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_conll(path: str):\n",
    "    \n",
    "    sentences, tags = [], []\n",
    "    cur_tokens, cur_tags = [], []\n",
    "    \n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if cur_tokens:\n",
    "                    sentences.append(cur_tokens); tags.append(cur_tags)\n",
    "                    cur_tokens, cur_tags = [], []\n",
    "                continue\n",
    "            \n",
    "            # each line: \"<token> <tag>\"\n",
    "            parts = line.split()\n",
    "            token = parts[0]\n",
    "            tag = parts[-1]\n",
    "            cur_tokens.append(token)\n",
    "            cur_tags.append(tag)\n",
    "            \n",
    "    if cur_tokens:\n",
    "        sentences.append(cur_tokens); tags.append(cur_tags)\n",
    "        \n",
    "    return sentences, tags\n",
    "\n",
    "train_sents, train_tags = read_conll(os.path.join(DATA_DIR, \"train.conll\"))\n",
    "val_sents,   val_tags   = read_conll(os.path.join(DATA_DIR, \"validation.conll\"))\n",
    "test_sents,  test_tags  = read_conll(os.path.join(DATA_DIR, \"test.conll\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbc46467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in HF datasets\n",
    "def to_hf_dataset(sents, tags):\n",
    "    return Dataset.from_dict({\"tokens\": sents, \"ner_tags\": tags})\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": to_hf_dataset(train_sents, train_tags),\n",
    "    \"validation\": to_hf_dataset(val_sents, val_tags),\n",
    "    \"test\": to_hf_dataset(test_sents, test_tags),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4916aff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-FACTORY_LOCATION', 'O', 'B-DOWNTIME_CAUSE', 'I-DOWNTIME_CAUSE', 'O', 'O', 'O', 'O', 'B-MACHINE_TYPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCTION_VOLUME', 'I-PRODUCTION_VOLUME', 'O', 'O', 'O', 'O']\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "ner_tags = data['train'][0]['ner_tags']\n",
    "print(ner_tags)\n",
    "print(len(ner_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2c6f5",
   "metadata": {},
   "source": [
    "### Model Building (Tokenizer and model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125446b",
   "metadata": {},
   "source": [
    "#### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e7f3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc29c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6d44d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Manitoba', 'experienced', 'reservoir', 'depletion', 'in', 'May', '2024', ';', 'intertie', 'capacity', 'tightened', 'and', 'authorities', 'recorded', 'a', '6.4%', 'decrease', 'relative', 'to', 'normal', 'levels.']\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "tokens = data['train'][0]['tokens']#original tokens \n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52df1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data['train'][0]['tokens']\n",
    "inputs = tokenizer(inputs, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "976a73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 10391, 4531, 10462, 1260, 7136, 2116, 1107, 1318, 17881, 1527, 132, 9455, 9570, 3211, 7974, 1105, 3912, 1802, 170, 127, 119, 125, 110, 9711, 5236, 1106, 2999, 3001, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'Manitoba', 'experienced', 'reservoir', 'de', '##ple', '##tion', 'in', 'May', '202', '##4', ';', 'inter', '##tie', 'capacity', 'tightened', 'and', 'authorities', 'recorded', 'a', '6', '.', '4', '%', 'decrease', 'relative', 'to', 'normal', 'levels', '.', '[SEP]']\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(inputs.tokens())\n",
    "print(len(inputs.tokens()) - 2 )#removing [CLS] and [SEP ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43580529",
   "metadata": {},
   "source": [
    "- Here , we can analyse that after tokenizer our single tokens are divided into multiple (like depletion is \n",
    "divided into de, ##ple, ##tion).\n",
    "- So, now we need to solve this misalignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd47be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 15, 15, 15, 15, 16, 17, 18, 19, 20, 20, None]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccb9b7",
   "metadata": {},
   "source": [
    "- We can see words ids (like 3,3,3) i.e this means for the single token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9abff",
   "metadata": {},
   "source": [
    "#### Preprocessing for Misaligned Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec6cb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    \n",
    "    for word_id in word_ids:\n",
    "        \n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "            \n",
    "        elif word_id != current_word:\n",
    "            current_word = word_id\n",
    "            new_labels.append(label2id[labels[word_id]])\n",
    "            \n",
    "        else:\n",
    "            new_labels.append(-100)#-100 means Don’t compute loss for this position.\n",
    "            #i.e ignore_index=-100 in PyTorch’s CrossEntropyLoss.\n",
    "            \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fdcdb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-FACTORY_LOCATION', 'O', 'B-DOWNTIME_CAUSE', 'I-DOWNTIME_CAUSE', 'O', 'O', 'O', 'O', 'B-MACHINE_TYPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCTION_VOLUME', 'I-PRODUCTION_VOLUME', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 15, 15, 15, 15, 16, 17, 18, 19, 20, 20, None]\n"
     ]
    }
   ],
   "source": [
    "labels = data['train'][0]['ner_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(word_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "150e212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 1, 0, 5, 6, -100, -100, 0, 0, 0, -100, 0, 3, -100, 0, 0, 0, 0, 0, 0, 7, -100, -100, -100, 8, 0, 0, 0, 0, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "new_labels_ids = align_labels_with_tokens(labels,word_ids)\n",
    "print(new_labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(batch):\n",
    "    \n",
    "    # print(batch)\n",
    "    \n",
    "    tokenized_inputs = tokenizer(batch[\"tokens\"], is_split_into_words=True)\n",
    "    label_ids = []\n",
    "    \n",
    "    for i, labels in enumerate(batch[\"ner_tags\"]):\n",
    "        \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = align_labels_with_tokens(labels, word_ids)#new label ids\n",
    "        label_ids.append(aligned_labels)\n",
    "        \n",
    "    # print(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    # print(f'Tokenized input : {tokenized_inputs}')\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d2456fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0ce71f909a47eab7dc648311346be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ece602c50546fe864db6e5c0bb1c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fc6283918b4cf1bbbd2e4f274d32db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_datasets = data.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, remove_columns=data[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ed416d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 110\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752aaa5",
   "metadata": {},
   "source": [
    "For classification we only need input_ids, attention_mask and labels.So, columns can be removed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9dd34",
   "metadata": {},
   "source": [
    "#### Data collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92526a0c",
   "metadata": {},
   "source": [
    "Use data collator for dynamic padding for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66a68c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "208da147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2b88c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10391,  4531, 10462,  1260,  7136,  2116,  1107,  1318, 17881,\n",
       "          1527,   132,  9455,  9570,  3211,  7974,  1105,  3912,  1802,   170,\n",
       "           127,   119,   125,   110,  9711,  5236,  1106,  2999,  3001,   119,\n",
       "           102,     0,     0,     0],\n",
       "        [  101,  1130,  1351, 17881,  1527,   117,  3717,  8866,  9298,  6024,\n",
       "          6122,  3208,  1105,  4533,   170,  1969, 13277, 13757,  1106, 19428,\n",
       "         21225,   177, 19694,  9447,  5611,  1120,  1103,  3070,  1114,  1103,\n",
       "          1244,  1311,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100,    1,    0,    5,    6, -100, -100,    0,    0,    0, -100,    0,\n",
       "            3, -100,    0,    0,    0,    0,    0,    0,    7, -100, -100, -100,\n",
       "            8,    0,    0,    0,    0, -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    0,    0, -100, -100,    1,    0,    0,    0,    5,    6,\n",
       "            0,    0,    0,    7,    8,    8,    0,    0, -100,    3, -100, -100,\n",
       "            0,    0,    0,    0,    0,    0,    1,    2, -100, -100]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "376b1832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
