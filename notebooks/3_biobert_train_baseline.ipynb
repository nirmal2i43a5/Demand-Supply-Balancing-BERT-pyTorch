{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210273f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nirma\\anaconda3\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\nirma\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\nirma\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\nirma\\anaconda3\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: seqeval in c:\\users\\nirma\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from seqeval) (1.5.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nirma\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers datasets accelerate evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b071c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from evaluate import load as load_metric\n",
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb92195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = \"../data\"  \n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.0\"\n",
    "OUTPUT_DIR = \"../outputs\"\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8065c",
   "metadata": {},
   "source": [
    "#### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4481ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = load_from_disk(\"../data/processed/bio_ner_bc5cdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa3d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_list: List[str] = [l.strip() for l in open(os.path.join(DATA_DIR, \"labels.txt\"), encoding=\"utf-8\")]\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1195400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_datasets = load_from_disk(\"../data/processed/bio_ner_bc5cdr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f49b3120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                                                        MODEL_NAME, \n",
    "                                                         id2label=id2label,\n",
    "                                                        label2id=label2id\n",
    "                                                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66116fec",
   "metadata": {},
   "source": [
    "#### Data collator (dynamic padding for token classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd6ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ca4f454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53d28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored indices (-100) & convert to label strings\n",
    "    true_labels, true_preds = [], []\n",
    "    for pred, lab in zip(preds, labels):\n",
    "        cur_true_labels, cur_true_preds = [], []\n",
    "        for p, l in zip(pred, lab):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            cur_true_labels.append(id2label[l])\n",
    "            cur_true_preds.append(id2label[p])\n",
    "        true_labels.append(cur_true_labels)\n",
    "        true_preds.append(cur_true_preds)\n",
    "\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    \n",
    "     # Aggregate main metrics\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04556620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "                        output_dir = OUTPUT_DIR  + \"/biobert-ner-checkpoints\",#finetuned ner \n",
    "                           eval_strategy=\"epoch\",\n",
    "                        save_strategy=\"epoch\",\n",
    "                        load_best_model_at_end=True,\n",
    "                        metric_for_best_model=\"f1\",\n",
    "                        greater_is_better=True,#indicate for higher f1   \n",
    "                        learning_rate=2e-5,\n",
    "                        per_device_train_batch_size=16,\n",
    "                        per_device_eval_batch_size=16,\n",
    "                        num_train_epochs=5,\n",
    "                        weight_decay=0.01,\n",
    "                        fp16=torch.cuda.is_available(),\n",
    "                        report_to=\"none\", # disable MLflow logging\n",
    "                        seed=42,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "943812e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.ner_metrics import compute_metrics\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args= args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # tokenizer=tokenizer,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # stop if val doesn't improve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a164887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4648\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4657\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f560fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nirma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1455' max='1455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1455/1455 11:40:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114875</td>\n",
       "      <td>0.775105</td>\n",
       "      <td>0.781755</td>\n",
       "      <td>0.778416</td>\n",
       "      <td>0.959687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.121630</td>\n",
       "      <td>0.790940</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.806689</td>\n",
       "      <td>0.960597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.123983</td>\n",
       "      <td>0.834034</td>\n",
       "      <td>0.841026</td>\n",
       "      <td>0.837515</td>\n",
       "      <td>0.966783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.140051</td>\n",
       "      <td>0.826364</td>\n",
       "      <td>0.857495</td>\n",
       "      <td>0.841642</td>\n",
       "      <td>0.966978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.144164</td>\n",
       "      <td>0.835845</td>\n",
       "      <td>0.858679</td>\n",
       "      <td>0.847108</td>\n",
       "      <td>0.967433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nirma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\nirma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\nirma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\nirma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17h 40min 19s\n",
      "Wall time: 11h 40min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1455, training_loss=0.06294214766459777, metrics={'train_runtime': 42040.4194, 'train_samples_per_second': 0.553, 'train_steps_per_second': 0.035, 'total_flos': 965983150460640.0, 'train_loss': 0.06294214766459777, 'epoch': 5.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cc077c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest_checkpoint = OUTPUT_DIR + \"/bert-ner-checkpoints/checkpoint-1455\"\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(latest_checkpoint)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(latest_checkpoint)\n",
    "\n",
    "# os.makedirs(OUTPUT_DIR + \"/models/bert_ner_baseline_v1\", exist_ok=True)\n",
    "\n",
    "# model.save_pretrained(OUTPUT_DIR + \"/models/bert_ner_baseline_v1\")\n",
    "# tokenizer.save_pretrained(OUTPUT_DIR + \"/models/bert_ner_baseline_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "trainer.save_model(OUTPUT_DIR + \"/models/biobert_ner_baseline_v1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de954177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 38min 29s\n",
      "Wall time: 7min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "# print(\"Test:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57168786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1530819982290268,\n",
       " 'eval_precision': 0.829612330686595,\n",
       " 'eval_recall': 0.8598954298993029,\n",
       " 'eval_f1': 0.8444824799125183,\n",
       " 'eval_accuracy': 0.9656873413130214,\n",
       " 'eval_runtime': 446.0397,\n",
       " 'eval_samples_per_second': 10.898,\n",
       " 'eval_steps_per_second': 0.682,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
