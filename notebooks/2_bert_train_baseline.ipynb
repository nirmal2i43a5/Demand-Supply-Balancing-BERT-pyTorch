{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210273f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets accelerate evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from evaluate import load as load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb92195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = \"../data\"  \n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"  \n",
    "OUTPUT_DIR = \"../outputs\"\n",
    "sys.path.append(os.path.abspath(\"..\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8065c",
   "metadata": {},
   "source": [
    "#### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4481ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "data = load_from_disk(\"../data/processed/bio_ner_bc5cdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_list: List[str] = [l.strip() for l in open(os.path.join(DATA_DIR, \"labels.txt\"), encoding=\"utf-8\")]\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1195400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_datasets = load_from_disk(\"../data/processed/bio_ner_bc5cdr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b3120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                                                        MODEL_NAME, \n",
    "                                                         id2label=id2label,\n",
    "                                                        label2id=label2id\n",
    "                                                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66116fec",
   "metadata": {},
   "source": [
    "#### Data collator (dynamic padding for token classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) \n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a6efd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=BertTokenizerFast(name_or_path='dmis-lab/biobert-base-cased-v1.1', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694c4ef",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e53d28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored indices (-100) & convert to label strings\n",
    "    true_labels, true_preds = [], []\n",
    "    for pred, lab in zip(preds, labels):\n",
    "        cur_true_labels, cur_true_preds = [], []\n",
    "        for p, l in zip(pred, lab):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            cur_true_labels.append(id2label[l])\n",
    "            cur_true_preds.append(id2label[p])\n",
    "        true_labels.append(cur_true_labels)\n",
    "        true_preds.append(cur_true_preds)\n",
    "\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    \n",
    "     # Aggregate main metrics\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04556620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "                        output_dir = OUTPUT_DIR  + \"/bert-ner-checkpoints\",#finetuned ner\n",
    "\n",
    "                        eval_strategy=\"epoch\",\n",
    "                        save_strategy=\"epoch\",\n",
    "                        load_best_model_at_end=True,\n",
    "                        metric_for_best_model=\"f1\",\n",
    "                        greater_is_better=True,#indicate for higher f1   \n",
    "                        learning_rate=2e-5,\n",
    "                        per_device_train_batch_size=16,\n",
    "                        per_device_eval_batch_size=16,\n",
    "                        num_train_epochs=5,\n",
    "                        weight_decay=0.01,\n",
    "                        fp16=torch.cuda.is_available(),\n",
    "                        report_to=\"none\", # disable MLflow/W&B logging\n",
    "                        seed=42,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "943812e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args= args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # tokenizer=tokenizer,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # stop if val doesn't improve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a164887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4648\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4657\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a0ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_866df\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_866df_level0_col0\" class=\"col_heading level0 col0\" >Epoch</th>\n",
       "      <th id=\"T_866df_level0_col1\" class=\"col_heading level0 col1\" >Training Loss</th>\n",
       "      <th id=\"T_866df_level0_col2\" class=\"col_heading level0 col2\" >Validation Loss</th>\n",
       "      <th id=\"T_866df_level0_col3\" class=\"col_heading level0 col3\" >Precision</th>\n",
       "      <th id=\"T_866df_level0_col4\" class=\"col_heading level0 col4\" >Recall</th>\n",
       "      <th id=\"T_866df_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_866df_level0_col6\" class=\"col_heading level0 col6\" >Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_866df_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_866df_row0_col1\" class=\"data row0 col1\" >No log</td>\n",
       "      <td id=\"T_866df_row0_col2\" class=\"data row0 col2\" >0.096700</td>\n",
       "      <td id=\"T_866df_row0_col3\" class=\"data row0 col3\" >0.830412</td>\n",
       "      <td id=\"T_866df_row0_col4\" class=\"data row0 col4\" >0.873570</td>\n",
       "      <td id=\"T_866df_row0_col5\" class=\"data row0 col5\" >0.851444</td>\n",
       "      <td id=\"T_866df_row0_col6\" class=\"data row0 col6\" >0.969178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_866df_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_866df_row1_col1\" class=\"data row1 col1\" >0.043200</td>\n",
       "      <td id=\"T_866df_row1_col2\" class=\"data row1 col2\" >0.121874</td>\n",
       "      <td id=\"T_866df_row1_col3\" class=\"data row1 col3\" >0.837147</td>\n",
       "      <td id=\"T_866df_row1_col4\" class=\"data row1 col4\" >0.891223</td>\n",
       "      <td id=\"T_866df_row1_col5\" class=\"data row1 col5\" >0.863339</td>\n",
       "      <td id=\"T_866df_row1_col6\" class=\"data row1 col6\" >0.968235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_866df_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_866df_row2_col1\" class=\"data row2 col1\" >0.043200</td>\n",
       "      <td id=\"T_866df_row2_col2\" class=\"data row2 col2\" >0.117825</td>\n",
       "      <td id=\"T_866df_row2_col3\" class=\"data row2 col3\" >0.879627</td>\n",
       "      <td id=\"T_866df_row2_col4\" class=\"data row2 col4\" >0.883531</td>\n",
       "      <td id=\"T_866df_row2_col5\" class=\"data row2 col5\" >0.881574</td>\n",
       "      <td id=\"T_866df_row2_col6\" class=\"data row2 col6\" >0.973479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_866df_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_866df_row3_col1\" class=\"data row3 col1\" >0.013200</td>\n",
       "      <td id=\"T_866df_row3_col2\" class=\"data row3 col2\" >0.127589</td>\n",
       "      <td id=\"T_866df_row3_col3\" class=\"data row3 col3\" >0.870946</td>\n",
       "      <td id=\"T_866df_row3_col4\" class=\"data row3 col4\" >0.895168</td>\n",
       "      <td id=\"T_866df_row3_col5\" class=\"data row3 col5\" >0.882891</td>\n",
       "      <td id=\"T_866df_row3_col6\" class=\"data row3 col6\" >0.973728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_866df_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_866df_row4_col1\" class=\"data row4 col1\" >0.013200</td>\n",
       "      <td id=\"T_866df_row4_col2\" class=\"data row4 col2\" >0.133697</td>\n",
       "      <td id=\"T_866df_row4_col3\" class=\"data row4 col3\" >0.874121</td>\n",
       "      <td id=\"T_866df_row4_col4\" class=\"data row4 col4\" >0.895069</td>\n",
       "      <td id=\"T_866df_row4_col5\" class=\"data row4 col5\" >0.884471</td>\n",
       "      <td id=\"T_866df_row4_col6\" class=\"data row4 col6\" >0.973955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23ccb87ef30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c496f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = OUTPUT_DIR + \"/bert-ner-checkpoints/checkpoint-582\"\n",
    "\n",
    "# trainer.train(resume_from_checkpoint=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "trainer.save_model(OUTPUT_DIR + \"/models/bert_ner_baseline_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de954177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nirma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='304' max='304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [304/304 09:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 55min 12s\n",
      "Wall time: 9min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8df79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test:\", test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
